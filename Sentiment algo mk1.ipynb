{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "from keras.preprocessing.sequence import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Time to manage data: 4.648736953735352\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "#vars\n",
    "vocab_size = 5000\n",
    "embedding_size = 128\n",
    "max_words = 50\n",
    "\n",
    "LSTM_units = 128\n",
    "Dense_units = 1\n",
    "\n",
    "start_data = time.time()\n",
    "\n",
    "#data\n",
    "data = pd.read_csv('train.csv', encoding='latin-1')\n",
    "\n",
    "#initialize tokenizer\n",
    "t = Tokenizer(num_words = vocab_size, split = ' ')\n",
    "t.fit_on_texts(data['SentimentText'])\n",
    "\n",
    "#divide into test and training\n",
    "X_train = data[:int(data.shape[0]*0.8)].SentimentText\n",
    "X_test = data[int(data.shape[0]*.8):].SentimentText\n",
    "\n",
    "Y_train = data[:int(data.shape[0]*0.8)].Sentiment\n",
    "Y_test = data[int(data.shape[0]*.8):].Sentiment\n",
    "\n",
    "#convert string to int\n",
    "X_train = t.texts_to_sequences(X_train)\n",
    "X_test = t.texts_to_sequences(X_test)\n",
    "\n",
    "#make every tweet the same length\n",
    "X_train = sequence.pad_sequences(X_train, max_words)\n",
    "X_test = sequence.pad_sequences(X_test, max_words)\n",
    "\n",
    "end_data = time.time()\n",
    "print('======================================')\n",
    "print('Time to manage data: ' + str(end_data - start_data))\n",
    "print('======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63992 samples, validate on 15999 samples\n",
      "Epoch 1/1\n",
      "63992/63992 [==============================] - 88s 1ms/step - loss: 7.1767 - acc: 0.5498 - val_loss: 6.5448 - val_acc: 0.5895\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(LSTM_units, dropout = .2, recurrent_dropout = .2))\n",
    "model.add(Dense(Dense_units, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size = 64, epochs = 1, validation_split = 0.20, shuffle = True, verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
